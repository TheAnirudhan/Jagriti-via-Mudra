{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from video_processor import VideoProcessor\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../datasets/UCF101/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_name</th>\n",
       "      <th>clip_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_Swing_g05_c02</td>\n",
       "      <td>../datasets/UCF101//train/Swing/v_Swing_g05_c0...</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_Swing_g21_c03</td>\n",
       "      <td>../datasets/UCF101//train/Swing/v_Swing_g21_c0...</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_Swing_g07_c01</td>\n",
       "      <td>../datasets/UCF101//train/Swing/v_Swing_g07_c0...</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_Swing_g24_c04</td>\n",
       "      <td>../datasets/UCF101//train/Swing/v_Swing_g24_c0...</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_Swing_g20_c03</td>\n",
       "      <td>../datasets/UCF101//train/Swing/v_Swing_g20_c0...</td>\n",
       "      <td>Swing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         clip_name                                          clip_path  label\n",
       "0  v_Swing_g05_c02  ../datasets/UCF101//train/Swing/v_Swing_g05_c0...  Swing\n",
       "1  v_Swing_g21_c03  ../datasets/UCF101//train/Swing/v_Swing_g21_c0...  Swing\n",
       "2  v_Swing_g07_c01  ../datasets/UCF101//train/Swing/v_Swing_g07_c0...  Swing\n",
       "3  v_Swing_g24_c04  ../datasets/UCF101//train/Swing/v_Swing_g24_c0...  Swing\n",
       "4  v_Swing_g20_c03  ../datasets/UCF101//train/Swing/v_Swing_g20_c0...  Swing"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_DIR+ 'train.csv')\n",
    "df['clip_path'] = df['clip_path'].apply(lambda x: DATASET_DIR + x) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Swing', 'SkyDiving', 'BreastStroke', 'TableTennisShot', 'FloorGymnastics', 'Rowing', 'SoccerJuggling', 'BenchPress', 'TennisSwing', 'FrisbeeCatch', 'Kayaking', 'ApplyLipstick', 'RopeClimbing', 'JumpingJack', 'BlowingCandles', 'BabyCrawling', 'HammerThrow', 'BoxingPunchingBag', 'Skiing', 'UnevenBars',\n",
       "       'PullUps', 'RockClimbingIndoor', 'HeadMassage', 'HulaHoop', 'Rafting', 'ShavingBeard', 'LongJump', 'FieldHockeyPenalty', 'VolleyballSpiking', 'Punch', 'Archery', 'PlayingDhol', 'CleanAndJerk', 'PlayingDaf', 'GolfSwing', 'PlayingSitar', 'IceDancing', 'SkateBoarding', 'BodyWeightSquats', 'HandstandPushups',\n",
       "       'BalanceBeam', 'WallPushups', 'HorseRiding', 'Bowling', 'JumpRope', 'CuttingInKitchen', 'CliffDiving', 'PlayingGuitar', 'FrontCrawl', 'HandstandWalking', 'Fencing', 'Basketball', 'Typing', 'TaiChi', 'PlayingCello', 'Lunges', 'Shotput', 'Nunchucks', 'ApplyEyeMakeup', 'BaseballPitch', 'Mixing', 'PlayingTabla',\n",
       "       'BandMarching', 'MoppingFloor', 'PushUps', 'MilitaryParade', 'Drumming', 'BasketballDunk', 'Knitting', 'WalkingWithDog', 'StillRings', 'PlayingPiano', 'PizzaTossing', 'TrampolineJumping', 'Hammering', 'CricketBowling', 'YoYo', 'SumoWrestling', 'JugglingBalls', 'JavelinThrow', 'SoccerPenalty', 'Biking',\n",
       "       'HighJump', 'BlowDryHair', 'WritingOnBoard', 'CricketShot', 'Billiards', 'BoxingSpeedBag', 'SalsaSpin', 'ThrowDiscus', 'HorseRace', 'BrushingTeeth', 'PlayingViolin', 'Skijet', 'ParallelBars', 'Haircut', 'PommelHorse', 'PoleVault', 'Surfing', 'PlayingFlute', 'Diving'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punch_df = df[df['label']=='Punch'].reset_index(drop=True)\n",
    "box_p_bag_df = df[df['label']=='BoxingPunchingBag'].reset_index(drop=True)\n",
    "walking_df = df[df['label']=='WalkingWithDog'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir =  list(walking_df['clip_path']) + list(box_p_bag_df['clip_path']) + list(punch_df['clip_path'])\n",
    "train_label = list( walking_df['label']) + list(box_p_bag_df['label']) + list(punch_df['label'])\n",
    "\n",
    "\n",
    "len(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrence on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('models/yolov8s-pose.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: WalkingWithDog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/92 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m label \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m batch, skipped_id \u001b[38;5;241m=\u001b[39m \u001b[43mvp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch)[:,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,batch_size , \u001b[38;5;241m34\u001b[39m)\n\u001b[0;32m     11\u001b[0m class_keypoints_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m: batch\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: [label]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch)})\n",
      "File \u001b[1;32md:\\Final_year_project\\Jagriti-via-Mudra\\video_processor.py:84\u001b[0m, in \u001b[0;36mVideoProcessor.process_video_batch\u001b[1;34m(self, video_paths)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(video_paths), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing Videos\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, video_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(video_paths):\n\u001b[1;32m---> 84\u001b[0m         keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m keypoints\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_frames:\n",
      "File \u001b[1;32md:\\Final_year_project\\Jagriti-via-Mudra\\video_processor.py:66\u001b[0m, in \u001b[0;36mVideoProcessor.process_video\u001b[1;34m(self, video_path)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# print(\"Running inference:\", end='\\t')\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrack(\n\u001b[0;32m     61\u001b[0m         video_path, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m         imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m960\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m         tracker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytetrack.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, iou\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     64\u001b[0m     )\n\u001b[1;32m---> 66\u001b[0m     data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_result_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_keypoints(data)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeypoints\n",
      "File \u001b[1;32md:\\Final_year_project\\Jagriti-via-Mudra\\video_processor.py:20\u001b[0m, in \u001b[0;36mVideoProcessor._convert_result_to_string\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_result_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, result):\n\u001b[0;32m     19\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result):\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_frames:\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\yolov8\\lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\engine\\predictor.py:290\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\models\\yolo\\pose\\predict.py:35\u001b[0m, in \u001b[0;36mPosePredictor.postprocess\u001b[1;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs):\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return detection results for a given input image or list of images.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnon_max_suppression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miou\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43magnostic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magnostic_nms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_det\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\utils\\ops.py:219\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, rotated)\u001b[0m\n\u001b[0;32m    217\u001b[0m nm \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m nc \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m    218\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m nc  \u001b[38;5;66;03m# mask start index\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m xc \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mmi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m conf_thres  \u001b[38;5;66;03m# candidates\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Settings\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# min_wh = 2  # (pixels) minimum box width and height\u001b[39;00m\n\u001b[0;32m    223\u001b[0m time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m+\u001b[39m max_time_img \u001b[38;5;241m*\u001b[39m bs  \u001b[38;5;66;03m# seconds to quit after\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "vp = VideoProcessor(model,batch_size, max_frames=100)\n",
    "\n",
    "keypoints_df = pd.DataFrame()\n",
    "class_keypoints_df = pd.DataFrame()\n",
    "for train_df in [walking_df, box_p_bag_df]:  #, punch_df[0:1]\n",
    "    label = train_df['label'][0]\n",
    "    print(f\"Training: {label}\")\n",
    "    batch, skipped_id = vp.process_video_batch(train_df['clip_path'])\n",
    "    batch = np.array(batch)[:,0,:].reshape(-1,batch_size , 34)\n",
    "    class_keypoints_df = pd.DataFrame({ 'keypoints': batch.tolist(), 'label': [label]*len(batch)})\n",
    "    keypoints_df = pd.concat([keypoints_df, class_keypoints_df])\n",
    "    print(f'Skipped: {len(skipped_id)}')\n",
    "\n",
    "# keypoints_df.columns = ['keypoints', 'label']\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40, 34)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = batch.reshape((-1, 1 , 20, 34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 20, 34)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WalkingWithDog', 'WalkingWithDog']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[label]*len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20, 34)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(keypoints_df['keypoints'].tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'keypoint_dataset/keypoints_df_10frames_{timestamp}.csv'\n",
    "\n",
    "# Save the keypoints_df to the new file\n",
    "keypoints_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "BoxingPunchingBag    1210\n",
       "WalkingWithDog        900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from video_processor import VideoProcessor\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_DATASET_DIR = 'examples/videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('models/yolov8n-pose.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walking_df =  pd.DataFrame({'clip_path': [C_DATASET_DIR+'Walking.mp4'], 'label': ['walking']})\n",
    "punching_df =  pd.DataFrame({'clip_path': [C_DATASET_DIR+'Punching.mp4'], 'label': ['punching']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: walking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 1/1 [00:31<00:00, 31.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: punching\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 1/1 [00:32<00:00, 32.09s/it]\n"
     ]
    }
   ],
   "source": [
    "vp = VideoProcessor(model, max_frames=1000)\n",
    "\n",
    "keypoints_df = pd.DataFrame()\n",
    "for train_df in [walking_df, punching_df]:  #\n",
    "    class_keypoints_df = pd.DataFrame()\n",
    "    label = train_df['label'][0]\n",
    "    print(f\"Training: {label}\")\n",
    "    batch, skipped_id = vp.process_video_batch(train_df['clip_path'])\n",
    "    batch = np.array(batch)[:,0,:].reshape(-1, 20, 34)\n",
    "    class_keypoints_df = pd.DataFrame({ 'keypoints': batch.tolist(), 'label': [label]*len(batch)})\n",
    "    keypoints_df = pd.concat([keypoints_df, class_keypoints_df])    \n",
    "    \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATASET\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'keypoint_dataset/custom_dataset/keypoints_df_{timestamp}.csv'\n",
    "\n",
    "# Save the keypoints_df to the new file\n",
    "keypoints_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "walking     50\n",
       "punching    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
