{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from video_processor import VideoProcessor\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_result_to_string(result):\n",
    "    output = '['\n",
    "    for i, r in enumerate(result):\n",
    "        if i==20: break\n",
    "        output += r.tojson() + ','\n",
    "    output = output[:-1]+']'\n",
    "    return eval(output), r.orig_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_keypoints(data, orig_shape):\n",
    "  d={}\n",
    "\n",
    "  keypoints_batch =[]\n",
    "  batch_size=20\n",
    "  max_persons = 10\n",
    "  for i, frame in enumerate(data):\n",
    "    track_id_keypoints = []\n",
    "    for person_id in range(max_persons):\n",
    "      \n",
    "      if person_id>=len(frame):\n",
    "        track_id_keypoints = d.get(person_id, [])\n",
    "        track_id_keypoints.append(np.zeros(34))\n",
    "        d[person_id] = track_id_keypoints\n",
    "\n",
    "      else:\n",
    "          keypoints = frame[person_id][\"keypoints\"]\n",
    "          x = np.array(keypoints[\"x\"]) / orig_shape[1]\n",
    "          y = np.array(keypoints[\"y\"]) / orig_shape[0]\n",
    "          XY = np.asarray([j for j in zip(x, y)]).reshape(34)\n",
    "      \n",
    "          track_id_keypoints = d.get(person_id, [])\n",
    "          track_id_keypoints.append(XY)\n",
    "          d[person_id] = track_id_keypoints\n",
    "\n",
    "  return np.asarray(list(d.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = 'examples/videos/V12.mp4'\n",
    "# result = model.track(image_path, device='0', show=True, verbose=False, stream=True)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# data, orig_shape = _convert_result_to_string(result)\n",
    "\n",
    "# keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "\n",
    "# keypoints_batch.shape\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 10, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "from keras.models import load_model\n",
    "fusion_model = load_model('models/detector_combined.keras')\n",
    "fusion_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(data):\n",
    "    prediction = fusion_model.predict(data, verbose=False).tolist()\n",
    "    # classes = ['boxing', 'handwaving', 'jogging', 'running', 'walking', 'handclapping']\n",
    "    # p = []\n",
    "    # for i in prediction:\n",
    "    #     p.append([classes[i.index(max(i))], max(i)])\n",
    "    # return p\n",
    "    return fusion_model.predict(data, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.08961296081542969 seconds\n",
      "Execution time: 0.09252047538757324 seconds\n",
      "Execution time: 0.0930325984954834 seconds\n",
      "Execution time: 0.10413455963134766 seconds\n",
      "Execution time: 0.09251856803894043 seconds\n",
      "Execution time: 0.0955345630645752 seconds\n",
      "Execution time: 0.09652447700500488 seconds\n",
      "Execution time: 0.08450651168823242 seconds\n",
      "Execution time: 0.08659791946411133 seconds\n",
      "Execution time: 0.08352017402648926 seconds\n",
      "Execution time: 0.08457541465759277 seconds\n",
      "Execution time: 0.08351874351501465 seconds\n",
      "Execution time: 0.08581137657165527 seconds\n",
      "Error reading frame!\n"
     ]
    }
   ],
   "source": [
    "# Violence test\n",
    "import time\n",
    "model = YOLO('models/yolov8n-pose.pt') \n",
    "cap = cv2.VideoCapture(\"examples/videos/Das_Wunder_von_Bern_(Deutschland_2002_2003)_kick_ball_f_cm_np1_fr_goo_2.avi\")\n",
    "\n",
    "buffer_size = fusion_model.input_shape[1]\n",
    "keypoints_buffer = np.zeros((10,1,34))\n",
    "\n",
    "vp = VideoProcessor(model, max_frames=1, img_sz=320, show_stream=False)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # img_resp = requests.get(url) \n",
    "    # img_arr = np.array(bytearray(img_resp.content), dtype=np.uint8) \n",
    "    # img = cv2.imdecode(img_arr, -1) \n",
    "    \n",
    "    if ret == False:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        print(\"Error reading frame!\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    if keypoints_buffer.shape[1] != buffer_size:\n",
    "        keypoints_batch = vp.process_image(frame)\n",
    "        keypoints_buffer = np.concatenate((keypoints_buffer, keypoints_batch), axis=1)\n",
    "\n",
    "        # result = model.track(frame, persist=True, verbose=False, device='0')\n",
    "        # data, orig_shape = _convert_result_to_string(result)\n",
    "        # keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "        \n",
    "        # print(keypoints_buffer.shape)\n",
    "\n",
    "    else:\n",
    "\n",
    "        \n",
    "        keypoints_batch = vp.process_image(frame)\n",
    "\n",
    "\n",
    "        # result = model.track(frame, persist=True, verbose=False, device='0')\n",
    "        # data, orig_shape = _convert_result_to_string(result)\n",
    "        # keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "        \n",
    "        if keypoints_buffer.shape[1] + keypoints_batch.shape[1] > buffer_size:\n",
    "            excess_size = keypoints_buffer.shape[1] + keypoints_batch.shape[1] - buffer_size\n",
    "            keypoints_buffer = keypoints_buffer[:, excess_size:, :]\n",
    "\n",
    "        # Concatenate the new keypoints_batch to the circular buffer\n",
    "        keypoints_buffer = np.concatenate((keypoints_buffer, keypoints_batch), axis=1)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        prediction = predictor(keypoints_buffer)\n",
    "        \n",
    "        end_time = time.time()  # Stop the timer after the loop\n",
    "        execution_time = end_time - start_time\n",
    "        print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "        for id, person in enumerate(vp.data[0]):\n",
    "            x1 = int(person['box']['x1'])\n",
    "            y1 = int(person['box']['y1'])\n",
    "            x2 = int(person['box']['x2'])\n",
    "            y2 = int(person['box']['y2'])\n",
    "            # print((prediction[id][1]))\n",
    "            # print([(x1,y1), (x2,y2), prediction[id]], end='\\r')\n",
    "\n",
    "            color = (0,0,255) if prediction[id][0] > 0.7 else (0,255,0)             \n",
    "            frame = cv2.rectangle(frame, (x1,y1), (x2,y2), color, 5)\n",
    "            # frame = cv2.putText(frame, prediction[id][0],(x1,y1),fontFace=1, fontScale=2, color=color, thickness= 1)\n",
    "        \n",
    "\n",
    "    cv2.imshow('Output',frame)\n",
    "\n",
    "    if cv2.waitKey(1)==27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    " \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.08454632759094238 seconds\n",
      "Execution time: 0.0835261344909668 seconds\n",
      "Execution time: 0.08609628677368164 seconds\n",
      "Execution time: 0.08455085754394531 seconds\n",
      "Execution time: 0.08324599266052246 seconds\n",
      "Execution time: 0.08452200889587402 seconds\n",
      "Execution time: 0.08753395080566406 seconds\n",
      "Execution time: 0.08809304237365723 seconds\n",
      "Execution time: 0.08658027648925781 seconds\n",
      "Execution time: 0.09451603889465332 seconds\n",
      "Execution time: 0.09000730514526367 seconds\n",
      "Execution time: 0.08558130264282227 seconds\n",
      "Execution time: 0.08640766143798828 seconds\n",
      "Execution time: 0.0825202465057373 seconds\n",
      "Execution time: 0.08658027648925781 seconds\n",
      "Execution time: 0.08552122116088867 seconds\n",
      "Execution time: 0.0905153751373291 seconds\n",
      "Execution time: 0.08603072166442871 seconds\n",
      "Execution time: 0.08352184295654297 seconds\n",
      "Execution time: 0.08369565010070801 seconds\n",
      "Execution time: 0.08055257797241211 seconds\n",
      "Execution time: 0.08954215049743652 seconds\n",
      "Execution time: 0.07956433296203613 seconds\n",
      "Execution time: 0.08064532279968262 seconds\n",
      "Execution time: 0.0810089111328125 seconds\n",
      "Execution time: 0.08162927627563477 seconds\n",
      "Execution time: 0.08359551429748535 seconds\n",
      "Execution time: 0.09353923797607422 seconds\n",
      "Execution time: 0.08721709251403809 seconds\n",
      "Execution time: 0.08028030395507812 seconds\n",
      "Execution time: 0.0885157585144043 seconds\n",
      "Error reading frame!\n"
     ]
    }
   ],
   "source": [
    "# Violence test\n",
    "import time\n",
    "model = YOLO('models/yolov8n-pose.pt') \n",
    "cap = cv2.VideoCapture(\"examples/videos/likebeckam_walk_f_cm_np1_ri_med_31.avi\")\n",
    "\n",
    "buffer_size = fusion_model.input_shape[1]\n",
    "keypoints_buffer = np.zeros((10,1,34))\n",
    "\n",
    "vp = VideoProcessor(model, max_frames=1, img_sz=320, show_stream=False)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # img_resp = requests.get(url) \n",
    "    # img_arr = np.array(bytearray(img_resp.content), dtype=np.uint8) \n",
    "    # img = cv2.imdecode(img_arr, -1) \n",
    "    \n",
    "    if ret == False:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        print(\"Error reading frame!\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    if keypoints_buffer.shape[1] != buffer_size:\n",
    "        keypoints_batch = vp.process_image(frame)\n",
    "        keypoints_buffer = np.concatenate((keypoints_buffer, keypoints_batch), axis=1)\n",
    "\n",
    "        # result = model.track(frame, persist=True, verbose=False, device='0')\n",
    "        # data, orig_shape = _convert_result_to_string(result)\n",
    "        # keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "        \n",
    "        # print(keypoints_buffer.shape)\n",
    "\n",
    "    else:\n",
    "\n",
    "        \n",
    "        keypoints_batch = vp.process_image(frame)\n",
    "\n",
    "\n",
    "        # result = model.track(frame, persist=True, verbose=False, device='0')\n",
    "        # data, orig_shape = _convert_result_to_string(result)\n",
    "        # keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "        \n",
    "        if keypoints_buffer.shape[1] + keypoints_batch.shape[1] > buffer_size:\n",
    "            excess_size = keypoints_buffer.shape[1] + keypoints_batch.shape[1] - buffer_size\n",
    "            keypoints_buffer = keypoints_buffer[:, excess_size:, :]\n",
    "\n",
    "        # Concatenate the new keypoints_batch to the circular buffer\n",
    "        keypoints_buffer = np.concatenate((keypoints_buffer, keypoints_batch), axis=1)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        prediction = predictor(keypoints_buffer)\n",
    "        \n",
    "        end_time = time.time()  # Stop the timer after the loop\n",
    "        execution_time = end_time - start_time\n",
    "        print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "        for id, person in enumerate(vp.data[0]):\n",
    "            x1 = int(person['box']['x1'])\n",
    "            y1 = int(person['box']['y1'])\n",
    "            x2 = int(person['box']['x2'])\n",
    "            y2 = int(person['box']['y2'])\n",
    "            # print((prediction[id][1]))\n",
    "            # print([(x1,y1), (x2,y2), prediction[id]], end='\\r')\n",
    "\n",
    "            color = (0,0,255) if prediction[id][0] > 0.7 else (0,255,0)             \n",
    "            frame = cv2.rectangle(frame, (x1,y1), (x2,y2), color, 5)\n",
    "            # frame = cv2.putText(frame, prediction[id][0],(x1,y1),fontFace=1, fontScale=2, color=color, thickness= 1)\n",
    "        \n",
    "\n",
    "    cv2.imshow('Output',frame)\n",
    "\n",
    "    if cv2.waitKey(1)==27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    " \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.08224272727966309 seconds\n",
      "Execution time: 0.08563041687011719 seconds\n",
      "Execution time: 0.0875554084777832 seconds\n",
      "Execution time: 0.08800411224365234 seconds\n",
      "Execution time: 0.09051346778869629 seconds\n",
      "Execution time: 0.08654332160949707 seconds\n",
      "Execution time: 0.08651876449584961 seconds\n",
      "Execution time: 0.08407092094421387 seconds\n",
      "Execution time: 0.07957720756530762 seconds\n"
     ]
    }
   ],
   "source": [
    "# Non Violence sample\n",
    "import time\n",
    "model = YOLO('models/yolov8n-pose.pt') \n",
    "cap = cv2.VideoCapture(\"examples/videos/v_WalkingWithDog_g18_c04.avi\")\n",
    "\n",
    "buffer_size = fusion_model.input_shape[1]\n",
    "keypoints_buffer = np.zeros((10,1,34))\n",
    "\n",
    "vp = VideoProcessor(model, max_frames=1, img_sz=320, show_stream=False)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # img_resp = requests.get(url) \n",
    "    # img_arr = np.array(bytearray(img_resp.content), dtype=np.uint8) \n",
    "    # img = cv2.imdecode(img_arr, -1) \n",
    "    \n",
    "    if ret == False:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        print(\"Error reading frame!\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    if keypoints_buffer.shape[1] != buffer_size:\n",
    "        keypoints_batch = vp.process_image(frame)\n",
    "        keypoints_buffer = np.concatenate((keypoints_buffer, keypoints_batch), axis=1)\n",
    "\n",
    "        # result = model.track(frame, persist=True, verbose=False, device='0')\n",
    "        # data, orig_shape = _convert_result_to_string(result)\n",
    "        # keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "        \n",
    "        # print(keypoints_buffer.shape)\n",
    "\n",
    "    else:\n",
    "\n",
    "        \n",
    "        keypoints_batch = vp.process_image(frame)\n",
    "\n",
    "\n",
    "        # result = model.track(frame, persist=True, verbose=False, device='0')\n",
    "        # data, orig_shape = _convert_result_to_string(result)\n",
    "        # keypoints_batch = _extract_keypoints(data, orig_shape)\n",
    "        \n",
    "        if keypoints_buffer.shape[1] + keypoints_batch.shape[1] > buffer_size:\n",
    "            excess_size = keypoints_buffer.shape[1] + keypoints_batch.shape[1] - buffer_size\n",
    "            keypoints_buffer = keypoints_buffer[:, excess_size:, :]\n",
    "\n",
    "        # Concatenate the new keypoints_batch to the circular buffer\n",
    "        keypoints_buffer = np.concatenate((keypoints_buffer, keypoints_batch), axis=1)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        prediction = predictor(keypoints_buffer)\n",
    "        \n",
    "        end_time = time.time()  # Stop the timer after the loop\n",
    "        execution_time = end_time - start_time\n",
    "        print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "        for id, person in enumerate(vp.data[0]):\n",
    "            x1 = int(person['box']['x1'])\n",
    "            y1 = int(person['box']['y1'])\n",
    "            x2 = int(person['box']['x2'])\n",
    "            y2 = int(person['box']['y2'])\n",
    "            # print((prediction[id][1]))\n",
    "            # print([(x1,y1), (x2,y2), prediction[id]], end='\\r')\n",
    "\n",
    "            color = (0,0,255) if prediction[id][0] > 0.7 else (0,255,0)             \n",
    "            frame = cv2.rectangle(frame, (x1,y1), (x2,y2), color, 5)\n",
    "            # frame = cv2.putText(frame, prediction[id][0],(x1,y1),fontFace=1, fontScale=2, color=color, thickness= 1)\n",
    "        \n",
    "\n",
    "    cv2.imshow('Output',frame)\n",
    "\n",
    "    if cv2.waitKey(1)==27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break\n",
    "\n",
    " \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries \n",
    "import requests \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import imutils \n",
    "\n",
    "# Replace the below URL with your own. Make sure to add \"/shot.jpg\" at last. \n",
    "url = \"http://192.168.186.196:8000/stream.mjpg\"\n",
    "\n",
    "# While loop to continuously fetching data from the Url \n",
    "while True: \n",
    "\timg_resp = requests.get(url) \n",
    "\tprint(img_resp)\n",
    "\t# break\n",
    "\timg_arr = np.array(bytearray(img_resp.content), dtype=np.uint8) \n",
    "\timg = cv2.imdecode(img_arr, -1) \n",
    "\timg = imutils.resize(img, width=1000, height=1800) \n",
    "\tcv2.imshow(\"Android_cam\", img) \n",
    "\n",
    "\t# Press Esc key to exit \n",
    "\tif cv2.waitKey(1) == 27: \n",
    "\t\tbreak\n",
    "\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
